{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on:\n",
    "https://github.com/ml4a/ml4a-guides/blob/master/notebooks/recurrent_neural_networks.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks: Character RNNs with Keras\n",
    "\n",
    "Often we are not interested in isolated datapoints, but rather datapoints within a context of others. A datapoint may mean something different depending on what's come before it. This can typically be represented as some kind of _sequence_ of datapoints, perhaps the most common of which is a time series.\n",
    "\n",
    "One of the most ubiquitous sequences of data where context is especially important is natural language. We have quite a few words in English where the meaning of a word may be totally different depending on it's context. An innocuous example of this is \"bank\": \"I went fishing down by the river bank\" vs \"I deposited some money into the bank\".\n",
    "\n",
    "If we consider that each word is a datapoint, most non-recurrent methods will treat \"bank\" in the first sentence exactly the same as \"bank\" in the second sentence - they are indistinguishable. If you think about it, in isolation they are indistinguishable to us as well - it's the same word!\n",
    "\n",
    "We can only start to discern them when we consider the previous word (or words). So we might want our neural network to consider that \"bank\" in the first sentence is preceded by \"river\" and that in the second sentence \"money\" comes a few words before it. That's basically what RNNs do - they \"remember\" some of the previous context and that influences the output it produces. This \"memory\" (called the network's \"_hidden state_\") works by retaining some of the previous outputs and combining it with the current input; this recursing (feedback) of the network's output back into itself is where its name comes from.\n",
    "\n",
    "This recursing makes RNNs quite deep, and thus they can be difficult to train. The gradient gets smaller and smaller the deeper it is pushed backwards through the network until it \"vanishes\" (effectively becomes zero), so long-term dependencies are hard to learn. The typical practice is to only extend the RNN back a certain number of time steps so the network is still trainable.\n",
    "\n",
    "Certain units, such as the LSTM (long short-term memory) and GRU (gated recurrent unit), have been developed to mitigate some of this vanishing gradient effect.\n",
    "\n",
    "Let's walkthrough an example of a character RNN, which is a great approach for learning a character-level language model. A language model is essentially some function which returns a probability over possible words (or in this case, characters), based on what has been seen so far. This function can vary from region to region (e.g. if terms like \"pop\" are used more commonly than \"soda\") or from person to person. You could say that a (good) language model captures the style in which someone writes.\n",
    "\n",
    "Language models often must make the simplifying assumption that only what came immediately (one time step) before matters (this is called the \"Markov assumption\"), but with RNNs we do not need to make such an assumption.\n",
    "\n",
    "We'll use Keras which makes building neural networks extremely easy (this example is an annotated version of Keras's [LSTM text generation example](https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py)).\n",
    "\n",
    "First we'll do some simple preparation - import the classes we need and load up the text we want to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#if using Theano with GPU\n",
    "#os.environ[\"THEANO_FLAGS\"] = \"mode=FAST_RUN,device=gpu,floatX=float32\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Joke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[me narrating a documentary about narrators] \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Telling my daughter garlic is good for you. Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>I've been going through a really rough period ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>If I could have dinner with anyone, dead or al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Two guys walk into a bar. The third guy ducks.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                               Joke\n",
       "0   1  [me narrating a documentary about narrators] \"...\n",
       "1   2  Telling my daughter garlic is good for you. Go...\n",
       "2   3  I've been going through a really rough period ...\n",
       "3   4  If I could have dinner with anyone, dead or al...\n",
       "4   5     Two guys walk into a bar. The third guy ducks."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load up our text\n",
    "all_jokes = pd.read_csv('shortjokes.csv')\n",
    "all_jokes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[me narrating a documentary about narrators] \"I can\\'t hear what they\\'re saying cuz I\\'m talking\"\\nTelling my daughter garlic is good for you. Good immune system and keeps pests away.Ticks, mosquitos, vampires... men.\\nI\\'ve been going through a really rough period at work this week It\\'s my own fault for swapping my tampax for sand paper.\\nIf I could have dinner with anyone, dead or alive... ...I would choose alive. -B.J. Novak-\\nTwo guys walk into a bar. The third guy ducks.\\nWhy can\\'t Barbie get pregn'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us create a long string variable text\n",
    "text = '\\n'.join(all_jokes.Joke)\n",
    "text[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}']\n"
     ]
    }
   ],
   "source": [
    "# extract all (unique) characters\n",
    "# these are our \"categories\" or \"labels\". We want to predict the next character from the past few (e.g 20) characters\n",
    "\n",
    "def removeChars(text):\n",
    "    for char in ['\\x08', '\\x10', '~', '^']:\n",
    "        text = text.replace(char, '')\n",
    "    return text\n",
    "  \n",
    "all_jokes.Joke = all_jokes.Joke.apply(lambda x: removeChars(x))\n",
    "\n",
    "text = '\\n'.join(all_jokes.Joke)\n",
    "text[0:500]\n",
    "\n",
    "chars = list(set(text))\n",
    "\n",
    "all_jokes.Joke = all_jokes.Joke.apply(lambda x: '~' + x + '^')\n",
    "print(sorted(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define our RNN. Keras makes this trivial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(LSTM(512, return_sequences=True, input_shape=(max_len, len(chars))))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(LSTM(512, return_sequences=False))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(len(chars)))\n",
    "# model.add(Activation('softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're framing our task as a classification task. Given a sequence of characters, we want to predict the next character. We equate each character with some label or category (e.g. \"a\" is 0, \"b\" is 1, etc).\n",
    "\n",
    "We use the _softmax_ activation function on our output layer - this function is used for categorical output. It turns the output into a probability distribution over the categories (i.e. it makes the values the network outputs sum to 1). So the network will essentially tell us how strongly it feels about each character being the next one.\n",
    "\n",
    "The categorical cross-entropy loss the standard loss function for multilabel classification, which basically penalizes the network more the further off it is from the correct label.\n",
    "\n",
    "We use dropout here to prevent overfitting - we don't want the network to just return things already in the text, we want it to have some wiggle room and create novelty! Dropout is a technique where, in training, some percent (here, 20%) of random neurons of the associated layer are \"turned off\" for that epoch. This prevents overfitting by preventing the network from relying on particular neurons.\n",
    "\n",
    "That's it for the network architecture!\n",
    "\n",
    "To train, we have to do some additional preparation. We need to chop up the text into character sequences of the length we specified (`max_len`) - these are our training inputs. We match them with the character that immediately follows each sequence. These are our expected training outputs.\n",
    "\n",
    "For example, say we have the following text (this quote is from Zhuang Zi). With `max_len=20`, we could manually create the first couple training examples like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to map each character to a label and create a reverse mapping to use later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_labels = {ch:i+3 for i, ch in enumerate(chars)}\n",
    "labels_char = {i+3:ch for i, ch in enumerate(chars)}\n",
    "# Padding Char:\n",
    "char_labels['PAD'] = 0\n",
    "labels_char[0] = 'PAD'\n",
    "# Start Char\n",
    "char_labels['~'] = 1\n",
    "labels_char[1] = '~'\n",
    "# End Char\n",
    "char_labels['^'] = 2\n",
    "labels_char[2] = '^'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = sorted(list(char_labels.values()))\n",
    "for i, k in enumerate(z):\n",
    "    if i != k:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'3': 3, 'O': 4, 'Y': 5, 'F': 6, '}': 7, 'N': 8, 'V': 9, '-': 10, 'd': 11, 'X': 12, 'r': 13, 'w': 14, 's': 15, '4': 16, '0': 17, \"'\": 18, '(': 19, '_': 20, ' ': 21, 'E': 22, 'M': 23, 'L': 24, 'k': 25, 'D': 26, '>': 27, 'i': 28, 'x': 29, '{': 30, 'I': 31, '\"': 32, 'h': 33, 'j': 34, 'W': 35, 'T': 36, '$': 37, '`': 38, 'a': 39, '|': 40, 'R': 41, ';': 42, '<': 43, 'o': 44, 'Q': 45, ']': 46, 'K': 47, ':': 48, '=': 49, 't': 50, 'c': 51, '2': 52, 'g': 53, '.': 54, 'A': 55, 'q': 56, '\\\\': 57, '5': 58, '9': 59, 'U': 60, '[': 61, 'y': 62, '?': 63, 'f': 64, 'b': 65, '#': 66, 'u': 67, '*': 68, '+': 69, '8': 70, '/': 71, 'C': 72, 'z': 73, ')': 74, 'e': 75, '6': 76, 'G': 77, 'H': 78, 'p': 79, 'P': 80, 'B': 81, 'S': 82, '&': 83, '1': 84, '%': 85, 'Z': 86, ',': 87, 'm': 88, '\\n': 89, 'J': 90, 'n': 91, 'v': 92, '@': 93, 'l': 94, '7': 95, '!': 96, 'PAD': 0, '~': 1, '^': 2}\n"
     ]
    }
   ],
   "source": [
    "print(char_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start constructing our numerical input 3-tensor and output matrix. Each input example (i.e. a sequence of characters) is turned into a matrix of one-hot vectors; that is, a bunch of vectors where the index corresponding to the character is set to 1 and all the rest are set to zero.\n",
    "\n",
    "For example, if we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    }
   ],
   "source": [
    "print(len(char_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating X Data\n",
    "max_len = 200\n",
    "x_data = np.array([np.array([char_labels[char] for char in z[:-1]]) for z in all_jokes.Joke])\n",
    "# input_sequences = np.array(pad_sequences(x_data,   \n",
    "#                             maxlen=max_len, padding='post'))\n",
    "\n",
    "\n",
    "y_data = np.array([np.array([char_labels[char] for char in z[1:]]) for z in all_jokes.Joke])\n",
    "# true_output = np.array(pad_sequences(y_data,   \n",
    "#                             maxlen=max_len, padding='post'))\n",
    "\n",
    "\n",
    "# seq_lengths = np.array([len(z) for z in all_jokes.Joke])\n",
    "# max_len = 20\n",
    "# x_data = [[char_labels[char] for char in z] for z in all_jokes.Joke]\n",
    "# input_sequences = []\n",
    "# labels = []\n",
    "# for seq in tqdm(x_data):\n",
    "#   for i in range(len(seq)):\n",
    "#     if i+max_len < len(seq):\n",
    "#       input_sequences.append(seq[i:i+max_len])\n",
    "#       labels.append(seq[i+max_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((231657,), (231657,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape, y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "class CharRNN:\n",
    "    def __init__(self):\n",
    "        self.global_step = tf.train.get_or_create_global_step()\n",
    "        self.build_model()\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "        with tf.variable_scope('Initialization'):\n",
    "            self.x = tf.placeholder(tf.int32, shape=[None, None], name='Features')\n",
    "            self.y = tf.placeholder(tf.int32, shape=[None, None], name='Labels')\n",
    "            self.seq_lengths = tf.placeholder(tf.int32, shape=[None,], name='Sequence_Lengths')\n",
    "            \n",
    "            char_embeddings = tf.get_variable(\"char_embeddings\", [98, 36])\n",
    "            embedded_char_ids = tf.nn.embedding_lookup(char_embeddings, self.x)\n",
    "\n",
    "        with tf.variable_scope('LSTM'):\n",
    "            lstm1 = tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(num_units=256)\n",
    "#             lstm2 = tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(num_units=256)\n",
    "            \n",
    "            lstm1_output, _ = lstm1(tf.transpose(embedded_char_ids, [1,0,2]))\n",
    "#             lstm2_output, _ = lstm2(lstm1_output)\n",
    "\n",
    "            logits = tf.layers.dense(lstm1_output, units=97+1)\n",
    "            logits = tf.transpose(logits, [1,0,2])\n",
    "\n",
    "        with tf.variable_scope('Optimization'):\n",
    "            optimizer = tf.train.AdamOptimizer(0.001)\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=self.y)\n",
    "\n",
    "            self.loss = tf.reduce_mean(loss)\n",
    "            self.train_step = optimizer.minimize(self.loss, global_step=self.global_step)\n",
    "            \n",
    "        with tf.variable_scope('Decoding'):\n",
    "            start_toks = tf.fill([1], 1)\n",
    "            initial_state = tf.zeros(shape=[36,256])\n",
    "            decoder = tf.contrib.seq2seq.BeamSearchDecoder(cell=lstm1, \n",
    "                                              embedding=char_embeddings,\n",
    "                                              start_tokens=start_toks,\n",
    "                                              end_token=2,\n",
    "                                              initial_state=tf.zeros([128, 256], dtype=tf.float32),\n",
    "                                              beam_width=10)\n",
    "            \n",
    "            decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                                                          decoder = decoder,\n",
    "                                                          impute_finished = False,\n",
    "                                                          maximum_iterations = 200)\n",
    "            \n",
    "  \n",
    "def padDataBatch(data, labels):\n",
    "    seq_lengths = np.array([utter.shape[0] for utter in data])\n",
    "    maxlen = max(seq_lengths)\n",
    "    x_data = np.array([np.pad(utter, ((0, maxlen-utter.shape[0])), mode='constant') for utter in data], dtype=\"int32\")\n",
    "    y_data = np.array([np.pad(utter, ((0, maxlen-utter.shape[0])), mode='constant') for utter in labels], dtype=\"int32\")\n",
    "    to_categorical(x_data, num_classes=97)\n",
    "    return x_data, seq_lengths, y_data\n",
    "    \n",
    "    \n",
    "    \n",
    "def train(sess, model, num_epochs, input_seq, output_seq):\n",
    "    batch_size = 128\n",
    "    data_size = input_seq.shape[0]\n",
    "    all_idxs = np.arange(data_size)\n",
    "\n",
    "    for k in range(num_epochs):\n",
    "        np.random.shuffle(all_idxs)\n",
    "        i = 0\n",
    "        losses = []\n",
    "        for i in tqdm(range(0, len(all_idxs), batch_size)):\n",
    "            idxs = all_idxs[i:i+batch_size]\n",
    "\n",
    "            batch_data, batch_lens, batch_y = padDataBatch(input_seq[idxs], output_seq[idxs])\n",
    "            loss = sess.run([model.loss], feed_dict={model.x:batch_data, model.y:batch_y, \n",
    "                                                   model.seq_lengths:batch_lens})\n",
    "            losses.append(loss)\n",
    "            \n",
    "            \n",
    "        print(np.mean(losses))\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__call__() missing 1 required positional argument: 'state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-33ea4e56e773>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCharRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_to_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mchkpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_checkpoint_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logs/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msaver_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCheckpointSaverHook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'logs/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-5b4900f00255>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_or_create_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-5b4900f00255>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#             lstm2 = tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(num_units=256)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mlstm1_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_char_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;31m#             lstm2_output, _ = lstm2(lstm1_output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __call__() missing 1 required positional argument: 'state'"
     ]
    }
   ],
   "source": [
    "model = CharRNN()\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "chkpt = tf.train.get_checkpoint_state('logs/')\n",
    "saver_hook = tf.train.CheckpointSaverHook(checkpoint_dir='logs/', save_secs=100, saver=saver)\n",
    "with tf.train.SingularMonitoredSession(hooks=[saver_hook]) as sess:\n",
    "    loss = train(sess, model, 10, x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
